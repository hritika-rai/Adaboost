{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to read data from CSV files\n",
        "def read_data(filename):\n",
        "    X_data = np.genfromtxt(filename, delimiter=',', skip_header=1, usecols=(0, 1))\n",
        "    y_data = np.genfromtxt(filename, delimiter=',', skip_header=1, usecols=(2))\n",
        "    return X_data, y_data\n",
        "\n",
        "# Function for weak classifier training\n",
        "def weak_classifier(X, y, D):\n",
        "    # Combine features X and labels y into one array\n",
        "    X_sorted = np.hstack((X, y[:, np.newaxis]))\n",
        "    # Combine X_sorted and sample weights D\n",
        "    X_unsorted = np.hstack((X_sorted, D[:, np.newaxis]))\n",
        "    min_error = 10000\n",
        "    # Iterate over features for split search\n",
        "    for j in range(len(X[0])):\n",
        "        # Sort X_unsorted based on the current feature j\n",
        "        X_sorted = np.asarray(sorted(X_unsorted, key=lambda a: a[j]))\n",
        "        # Iterate over sorted samples to find best split\n",
        "        for i in range(len(X_sorted)):\n",
        "            left = X_sorted[X_sorted[:, j] <= X_sorted[i][j]]\n",
        "            right = X_sorted[X_sorted[:, j] > X_sorted[i][j]]\n",
        "            for l in range(2):\n",
        "                error = 0\n",
        "                errorcount = 0\n",
        "                maj_left = -1 if l == 0 else 1\n",
        "                maj_right = -1 * maj_left\n",
        "                for k in range(len(right)):\n",
        "                    error += right[k][3] if right[k][2] != maj_right else 0\n",
        "                    errorcount +=1 if right[k][2] != maj_right else 0\n",
        "                for k in range(len(left)):\n",
        "                    error += left[k][3] if left[k][2] != maj_left else 0\n",
        "                    errorcount += 1 if left[k][2] != maj_left else 0\n",
        "                if error < min_error:\n",
        "                    min_error = error\n",
        "                    best_split_value = X_sorted[i][j]\n",
        "                    best_split_feature = j\n",
        "                    left_class = maj_left\n",
        "                    right_class = maj_right\n",
        "    beta_t = 0.5 * np.log((1 - min_error) / min_error)\n",
        "    return beta_t, best_split_feature, best_split_value, left_class, right_class\n",
        "\n",
        "def update_weights(X, y, D, model_t):\n",
        "    # Extract model parameters\n",
        "    beta_t = model_t[0]\n",
        "    split_feat = model_t[1]\n",
        "    split_value = model_t[2]\n",
        "    majright = model_t[4]\n",
        "    majleft = model_t[3]\n",
        "\n",
        "    # Initialize arrays for intermediate computations\n",
        "    y_h = np.asarray([0] * len(X))[:, np.newaxis]\n",
        "    D_plus = np.asarray([0] * len(D))[:, np.newaxis]\n",
        "    Z_norm = 0\n",
        "\n",
        "    # Combine features X, labels y, and intermediate arrays into one array\n",
        "    X_sorted = np.hstack((np.hstack((X, y[:, np.newaxis])), y_h))\n",
        "    X_sorted = np.hstack((np.hstack((X_sorted, D[:, np.newaxis])), D_plus))\n",
        "\n",
        "    # Update sample weights based on weak classifier predictions\n",
        "    for i in range(len(X_sorted)):\n",
        "        if X_sorted[i][split_feat] <= split_value:\n",
        "            # Update class assignment based on split condition\n",
        "            X_sorted[i][3] = -1 if X_sorted[i][2] != majleft else 1\n",
        "        if X_sorted[i][split_feat] > split_value:\n",
        "            X_sorted[i][3] = -1 if X_sorted[i][2] != majright else 1\n",
        "\n",
        "    # Update sample weights and compute normalization factor Z_norm\n",
        "    for i in range(len(X_sorted)):\n",
        "        # Update sample weight using AdaBoost update rule\n",
        "        X_sorted[i][5] = (X_sorted[i][4] * np.exp(beta_t)) if X_sorted[i][3] == -1 else (X_sorted[i][4] * np.exp(-beta_t))\n",
        "        # Accumulate weights for normalization\n",
        "        Z_norm += (X_sorted[i][4] * np.exp(beta_t)) if X_sorted[i][3] == -1 else (X_sorted[i][4] * np.exp(-beta_t))\n",
        "\n",
        "    # Normalize the updated sample weights\n",
        "    return np.asarray(X_sorted[:, 5] / Z_norm)\n",
        "\n"
      ],
      "metadata": {
        "id": "UAZpJPz-04Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weak_predict(X_test, model):\n",
        "    # Initialize predictions list\n",
        "    y_pred = [0] * len(X_test)\n",
        "    # Extract model parameters\n",
        "    beta_t = model[0]\n",
        "    split_feature = int(model[1])\n",
        "    split_value = model[2]\n",
        "    maj_left = model[3]\n",
        "    maj_right = model[4]\n",
        "    # Make predictions for each sample in X_test\n",
        "    for i in range(len(X_test)):\n",
        "        y_pred[i] = maj_left if X_test[i][split_feature] <= split_value else maj_right\n",
        "    # Apply beta_t to predictions\n",
        "    return beta_t * np.asarray(y_pred)\n",
        "\n",
        "\n",
        "def adaboost_train(num_iter, X_train, y_train):\n",
        "    # Initialize list to store weak classifiers\n",
        "    hlist = []\n",
        "    # Initialize sample weights D\n",
        "    D = np.asarray([1/len(X_train)] * len(X_train))\n",
        "    # Iterate through num_iter rounds\n",
        "    for i in range(num_iter):\n",
        "        # Train weak classifier\n",
        "        wk_classifier = weak_classifier(X_train, y_train, D)\n",
        "        # Update sample weights D\n",
        "        D = update_weights(X_train, y_train, D, wk_classifier)\n",
        "        # Append weak classifier to hlist\n",
        "        hlist.append(wk_classifier)\n",
        "    return hlist\n"
      ],
      "metadata": {
        "id": "j1yoMKDs3UMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(X_test, y_test, hlist):\n",
        "    # Initialize error count\n",
        "    error = 0\n",
        "    # Initialize array for predictions\n",
        "    y_pred = np.asarray([0.0]*len(X_test))\n",
        "\n",
        "    # Aggregate predictions from all weak classifiers\n",
        "    for i in range(len(hlist)):\n",
        "        y_pred += weak_predict(X_test, hlist[i])\n",
        "\n",
        "    # Calculate error by comparing predictions to ground truth labels\n",
        "    for i in range(len(y_pred)):\n",
        "        error += 1 if np.sign(y_pred[i]) != y_test[i] else 0\n",
        "\n",
        "    # Calculate accuracy based on error count\n",
        "    accuracy = 1 - (error/len(y_pred))\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "-Z9w2Yz53PVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    X_train, y_train = read_data(\"/content/train_adaboost.csv\")\n",
        "    X_test, y_test = read_data(\"/content/test_adaboost.csv\")\n",
        "    h_list = np.asarray(adaboost_train(400, X_train, y_train))\n",
        "    accuracytest = []\n",
        "    for i in range(len(h_list)):\n",
        "        accuracytest.append(eval_model(X_test, y_test, h_list[:i+1]))\n",
        "    iters = np.linspace(1, 400, 400)\n",
        "    print(\"accuracy after training 400 weak classifiers: \", accuracytest[-1] * 100, \"%\")\n",
        "\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hqv2mQNg3Ml2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e10b41-14c6-4b5c-dcdc-1ff0de73ed4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy after training 400 weak classifiers:  97.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9epAm3Y0t2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}