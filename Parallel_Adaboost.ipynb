{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHbtRxDWB1bW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "# Function to read data from CSV files\n",
        "def read_data(filename):\n",
        "    X_data = np.genfromtxt ( filename, delimiter=',', skip_header=1, usecols=(0, 1))\n",
        "    y_data = np.genfromtxt ( filename, delimiter=',', skip_header=1, usecols=(2))\n",
        "    return X_data, y_data\n",
        "\n",
        "# Function to calculate error for each feature and split value\n",
        "def calculate_error(feature_index, X_sorted, min_error, best_split_value, best_split_feature, left_class, right_class):\n",
        "    for i in range(len(X_sorted)):\n",
        "        left = X_sorted[X_sorted[:, feature_index] <= X_sorted[i][feature_index]]\n",
        "        right = X_sorted[X_sorted[:, feature_index] > X_sorted[i][feature_index]]\n",
        "        for l in range(2):\n",
        "            error = 0\n",
        "            maj_left = -1 if l == 0 else 1\n",
        "            maj_right = -1 * maj_left\n",
        "            for k in range(len(right)):\n",
        "                error += right[k][3] if right[k][2] != maj_right else 0\n",
        "            for k in range(len(left)):\n",
        "                error += left[k][3] if left[k][2] != maj_left else 0\n",
        "            if error < min_error:\n",
        "                min_error = error\n",
        "                best_split_value = X_sorted[i][feature_index]\n",
        "                best_split_feature = feature_index\n",
        "                left_class = maj_left\n",
        "                right_class = maj_right\n",
        "    return min_error, best_split_value, best_split_feature, left_class, right_class\n",
        "\n",
        "# Function for weak classifier training\n",
        "def weak_classifier(X, y, D):\n",
        "    X_sorted = np.hstack((X, y[:, np.newaxis]))\n",
        "    X_unsorted = np.hstack((X_sorted, D[:, np.newaxis]))\n",
        "    min_error = 10000\n",
        "    # Define the number of parallel jobs (adjust as needed)\n",
        "    num_jobs = -1  # Use all available cores\n",
        "    results = Parallel(n_jobs=num_jobs)(\n",
        "        delayed(calculate_error)(j, np.asarray(sorted(X_unsorted, key=lambda a: a[j])), min_error, 0, 0, 0, 0)\n",
        "        for j in range(len(X[0]))\n",
        "    )\n",
        "    # Get the updated variables from the results\n",
        "    for result in results:\n",
        "        min_error, best_split_value, best_split_feature, left_class, right_class = result\n",
        "    beta_t = 0.5 * np.log((1 - min_error) / min_error)\n",
        "    return beta_t, best_split_feature, best_split_value, left_class, right_class\n",
        "\n",
        "# Function to update sample weights\n",
        "def update_weights(X, y, D, model_t):\n",
        "    beta_t = model_t[0]\n",
        "    split_feat = model_t[1]\n",
        "    split_value = model_t[2]\n",
        "    majright = model_t[4]\n",
        "    majleft = model_t[3]\n",
        "    y_h = np.asarray([0] * len(X))[:, np.newaxis]\n",
        "    D_plus = np.asarray([0] * len(D))[:, np.newaxis]\n",
        "    Z_norm = 0\n",
        "    X_sorted = np.hstack((np.hstack((X, y[:, np.newaxis])), y_h))\n",
        "    X_sorted = np.hstack((np.hstack((X_sorted, D[:, np.newaxis])), D_plus))\n",
        "\n",
        "    def update_weight_single(indices):\n",
        "        updated_weights = []\n",
        "        for i in indices:\n",
        "            if X_sorted[i][split_feat] <= split_value:\n",
        "                X_sorted[i][3] = -1 if X_sorted[i][2] != majleft else 1\n",
        "            if X_sorted[i][split_feat] > split_value:\n",
        "                X_sorted[i][3] = -1 if X_sorted[i][2] != majright else 1\n",
        "            weight = (X_sorted[i][4] * np.exp(beta_t)) if X_sorted[i][3] == -1 else (X_sorted[i][4] * np.exp(-beta_t))\n",
        "            updated_weights.append(weight)\n",
        "        return updated_weights\n",
        "\n",
        "    # Distribute workload in smaller chunks\n",
        "    chunk_size = len(X_sorted) // 10  # Adjust chunk size as needed\n",
        "    updated_weights_chunks = Parallel(n_jobs=-1)(\n",
        "        delayed(update_weight_single)(range(start, min(start + chunk_size, len(X_sorted)))) for start in range(0, len(X_sorted), chunk_size)\n",
        "    )\n",
        "\n",
        "    # Flatten the list of lists\n",
        "    updated_weights = [weight for sublist in updated_weights_chunks for weight in sublist]\n",
        "\n",
        "    for i in range(len(X_sorted)):\n",
        "        X_sorted[i][5] = updated_weights[i]\n",
        "        Z_norm += updated_weights[i]\n",
        "\n",
        "    return np.asarray(X_sorted[:, 5] / Z_norm)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weak_predict(X_test, model):\n",
        "    # Initialize predictions list\n",
        "    y_pred = [0] * len(X_test)\n",
        "    # Extract model parameters\n",
        "    beta_t = model[0]\n",
        "    split_feature = int(model[1])\n",
        "    split_value = model[2]\n",
        "    maj_left = model[3]\n",
        "    maj_right = model[4]\n",
        "    # Make predictions for each sample in X_test\n",
        "    for i in range(len(X_test)):\n",
        "        y_pred[i] = maj_left if X_test[i][split_feature] <= split_value else maj_right\n",
        "    # Apply beta_t to predictions\n",
        "    return beta_t * np.asarray(y_pred)\n",
        "\n",
        "\n",
        "def adaboost_train(num_iter, X_train, y_train):\n",
        "    # Initialize list to store weak classifiers\n",
        "    hlist = []\n",
        "    # Initialize sample weights D\n",
        "    D = np.asarray([1/len(X_train)] * len(X_train))\n",
        "    # Iterate through num_iter rounds\n",
        "    for i in range(num_iter):\n",
        "        # Train weak classifier\n",
        "        wk_classifier = weak_classifier(X_train, y_train, D)\n",
        "        # Update sample weights D\n",
        "        D = update_weights(X_train, y_train, D, wk_classifier)\n",
        "        # Append weak classifier to hlist\n",
        "        hlist.append(wk_classifier)\n",
        "    return hlist\n"
      ],
      "metadata": {
        "id": "5Q6ZKqJdCFsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(X_test, y_test, hlist):\n",
        "    # Define a function to predict using a single weak classifier\n",
        "    def predict_weak_classifier(X_test, h):\n",
        "        return weak_predict(X_test, h)\n",
        "\n",
        "    # Parallelize predictions for all weak classifiers in hlist\n",
        "    y_pred_all = Parallel(n_jobs=-1)(delayed(predict_weak_classifier)(X_test, h) for h in hlist)\n",
        "    # Aggregate predictions from all weak classifiers\n",
        "    y_pred = np.sum(y_pred_all, axis=0)\n",
        "\n",
        "    # Calculate error and accuracy\n",
        "    error = np.sum(np.sign(y_pred) != y_test)\n",
        "    accuracy = 1 - (error / len(y_test))\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "c0e9jWwaCTWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    start_time = time.time()  # Record start time\n",
        "    X_train, y_train = read_data(\"/content/train_adaboost.csv\")\n",
        "    X_test, y_test = read_data(\"/content/test_adaboost.csv\")\n",
        "    h_list = np.asarray(adaboost_train(400, X_train, y_train))\n",
        "    accuracytest = []\n",
        "    for i in range(len(h_list)):\n",
        "        accuracytest.append(eval_model(X_test, y_test, h_list[:i+1]))\n",
        "    end_time = time.time()\n",
        "    iters = np.linspace(1, 400, 400)\n",
        "    print(\"accuracy after training 400 weak classifiers: \", accuracytest[-1] * 100, \"%\")\n",
        "    print(\"Time taken:\", round((end_time - start_time) / 60, 2), \"minutes\")  # Print time taken in minutes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bemcWXx9CY2d",
        "outputId": "d3259be8-df6e-4ed3-ac88-703d17f4073a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy after training 400 weak classifiers:  76.0 %\n",
            "Time taken: 3.35 minutes\n"
          ]
        }
      ]
    }
  ]
}